### 카프카란?

---

- before Kafka

  ![a1](https://user-images.githubusercontent.com/41246605/223076735-bc0f5e85-e4ca-4bcd-bd23-f7ff638ae2fb.png)


- after kafka

![a2](https://user-images.githubusercontent.com/41246605/223076754-bffbb103-05cc-4730-a758-d06d5aa2ed56.png)


- Kafka의 기능

![a3](https://user-images.githubusercontent.com/41246605/223076784-f4f14fd6-e39c-4372-b8cb-32b75da00ab7.png)


- Apache Kafka 는 Source Application 과 Target Application 의 결합도를 낮추기 위해 탄생됨

- Source Application 에서 보내는 데이터의 포맷은 거의 제한이 없다.

- Kafka 는 각종 데이터를 담는 Topic 이 있는데, 쉽게 말하면 Queue 라고 볼 수 있다.

- Kafka 의 데이터를 넣는 역할은 Producer 인 Source Application 가 진행하고, Consumer 인 Target Application 이 데이터 처리를 진행합니다.

- 고가용성 장점을 제공

### 토픽이란?

---

- 토픽 : 카프카에서 데이터가 들어갈 수 있는 공간
- 카프카에서는 Topic 을 여러개 생성 가능
- Topic 은 큐의 역할을 함
- Kafka 는 목적에 따라 다양한 이름을 가질 수 있다.
    - ex) `click_log` , `send_sms` , `location_log`

### 파티션이란

---


![a4](https://user-images.githubusercontent.com/41246605/223076798-006f5508-db00-4a32-98dc-c24abe1c614c.png)


- 하나의 카프카는 여러 개의 파티션을 가질 수 있다.
- 하나의 파티션은 데이터가 인덱스 0부터 쌓이게 된다.
- 하나의 파티션은 데이터가 내부 끝에서부터 쌓인다.
- 파티션에 Consumer 가 붙게되면 데이터가 가장 오래된 순서부터 차례대로 가져가게 된다.
- 데이터가 들어오지 않으면 Consumer 는 다른 데이터가 들어올 때까지 대기하게 된다.
- Consumer 가 Partition 에서 데이터를 가져가더라도 데이터가 삭제되지 않고 그대로 남는다.
- 새로운 Consumer 가 붙었을 때 Partion 의 데이터를 다시 가져갈 수 있다.
    - 다만 Consumer Group 이 달라야 한다.
    - `auto.offset.reset=earliest` 여야 한다.
    - 이러한 구조로 Kafka 는 동일 데이터에 대해서 2번 처리할 수 있다.


### 파티션이 2개 이상인 경우

---

![a5](https://user-images.githubusercontent.com/41246605/223076804-2d239ab7-f1d6-4414-b218-fece5ff930df.png)

- 파티션이 2개 이상인 경우, Producer 가 데이터를 집어넣을 때 데이터의 키에 따라 적절한 파티션에 분배할 수 있다.
    - 키가 null 이고, 기본 파티셔너 사용할 경우 라운드 로빈(Round robin) 으로 할당한다.
    - 키가 있고, 기본 파티셔너 사용할 경우 키의 해시(hash) 값을 구하고, 특정 파티션에 할당한다.
- 파티션을 늘리는 것은 자유롭지만, 줄이는 것은 어렵다.
    - 파티션을 늘리면 컨슈머 갯수를 늘려서 데이터 분산 처리에 용이하다.
- 파티션의 record 는 언제 삭제되는지?
    - 삭제되는 타이밍은 옵션에 따라 다름
    - [`log.retention.ms`](http://log.retention.ms) : 최대 record 보존 시간
    - `log.retention.byte` : 최대 record 보존 크기 (byte)


### 데이터를 카프카로 전송하는 프로듀서

---

- 프로듀서는 데이터를 카프카에 보내는 역할을 함
- 엄청난 양의 클릭 로그들을 대량으로, 실시간으로 카프카에 적재할 때 프로듀서를 사용할 수 있음
- 프로듀서의 역할
    - Topic 에 해당하는 메시지를 생성
    - 특정 Topic 으로 데이터를 publish
    - 처리 실패/재시도

- gradle kafka dependency

```java
compile group: 'org.apache.kafka', name: 'kafka-clients', version: '2.3.0'
```

- kafka 는 broker 버전과 client 버전과의 호환성을 염두하고 사용해야 한다.
    - [https://blog.voidmainvoid.net/193](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqa0xWNG84REdzYmtGZ0x4X0ctRnlXWUJHb2tlUXxBQ3Jtc0tteUxobVM2b1BLT0VTUDEtc2RBckp4V2VUdzZEUWtnRVRfNTdXVXBEU2FpbGZPWm1pYnVIZGEydWEtMFpxeW5ZUzAtUnIzVGMxalVpN01OMWdpLTg4SFR1djdaYTNXaHFXOVpTOC1FTXBCbUNCWnprYw&q=https%3A%2F%2Fblog.voidmainvoid.net%2F193&v=aAu0FE3nvbk)

```java
public class Producer {
    public static void main(String[] args) throws IOException {
        Properties configs = new Properties();
        configs.put("bootstrap.servers", "localhost:9092");
        configs.put("key.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        configs.put("value.serializer", "org.apache.kafka.common.serialization.StringSerializer");
        
        KafkaProducer<String, String> producer = new KafkaProducer<String, String>(configs);
        ProducerRecord record = new ProducerRecord<String, String>("click_log", "login");
        producer.send(record);
        producer.close();
    }
}
```

- Properties 는 Producer 를 위한 설정을 함
    - bootstrap.servers 를 [localhost](http://localhost) 의 kafka 에 연동
    - kafka 의 주소목록은 되도록이면 2개 이상의 ip 와 port 를 설정하도록 권장
        - 둘 중 하나의 브로커가 비정상일 경우를 대비
    - key 와 value 에 대해 StringSerializer 를 통해 직렬화를 하였음
        - Byte array, String, Integer Serializer 를 사용할 수 있음
        - key 는 메시지를 보내면 토픽의 파티션이 지정될 때 사용됨
- config 를 통해 kafka 인스턴스를 생성
    - kafka 라이브러리에서는 ProducerRecord 를 제공함
    - ProducerRecord 를 통해 데이터를 어떤 파티션에 넣을 것인지 key 와 value 를 지정할 수 있음
- ProducerRecord 를 통해 키 없이 click_log 토픽에 login value 값을 넣습니다.
    - key 를 설정하고 싶다면 `new ProducerRecord<String, String>(”click_log”, “1”, “login”);` 으로설정할 수 있음
    - 파라미터 갯수에 따라 자동으로 오버로딩되어 인스턴스가 생성되므로 유의해서 생성해야함
- 데이터가 도착할 토픽, 데이터, 카프카 브로커의 호스트와 포트까지 정보를 모두 설정하였음


![a6](https://user-images.githubusercontent.com/41246605/223076812-10bc71e0-0ece-4d0f-bdc5-fc938c04c672.png)


- kafka 는 key 를 특정한 hash 값으로 변경시켜 파티션과 1:1 매칭을 시킴

- 토픽에 새로운 파티션을 추가하는 순간 키와 파티션의 매칭이 깨지기 때문에 키-파티션의 일관성이 보장되지 않음




